# AI Service

Enhanced AI-powered resume analysis and optimization service with OpenAI integration and vector embeddings.

## ğŸš€ Features

- **Semantic Job-Resume Matching**: Advanced matching using OpenAI embeddings and cosine similarity
- **Pinecone Vector Database Integration**: High-performance vector similarity search for semantic matching
- **AI-Powered Resume Optimization**: GPT-powered resume enhancement for specific job descriptions
- **Enhanced 3-Stage Optimization Pipeline**: Comprehensive analysis, rewriting, and quality validation
- **Redis Caching**: High-performance caching for OpenAI responses and embeddings (95%+ performance improvement)
- **Batch Job Analysis**: Process multiple job descriptions in parallel
- **Vector Embeddings Generation**: Generate and store embeddings for semantic search
- **Session Management**: Track processing sessions and progress
- **Quality Validation**: Confidence scoring and quality assessment
- **Database Persistence**: PostgreSQL storage for all operations and results
- **Health Monitoring**: Comprehensive health checks and metrics
- **Docker Support**: Complete containerized deployment

## ğŸ—ï¸ Architecture

```
AI Service
â”œâ”€â”€ Core AI Services
â”‚   â”œâ”€â”€ EnhancedAIService (Job matching, optimization)
â”‚   â”œâ”€â”€ EnhancedResumeOptimizer (3-stage pipeline)
â”‚   â”œâ”€â”€ PineconeService (Vector database operations)
â”‚   â”œâ”€â”€ EnhancedJobMatcher (Combined vector + semantic matching)
â”‚   â””â”€â”€ Vector Embeddings (OpenAI text-embedding-3-small)
â”œâ”€â”€ Database Layer
â”‚   â”œâ”€â”€ PostgreSQL (Primary storage & backup)
â”‚   â”œâ”€â”€ Redis (Caching & sessions)
â”‚   â””â”€â”€ Pinecone (Vector database for similarity search)
â”œâ”€â”€ API Layer
â”‚   â”œâ”€â”€ FastAPI (REST API)
â”‚   â”œâ”€â”€ Pydantic (Validation)
â”‚   â”œâ”€â”€ AI Routes (Core AI operations)
â”‚   â”œâ”€â”€ Pinecone Routes (Vector operations)
â”‚   â””â”€â”€ OpenAPI (Documentation)
â””â”€â”€ Infrastructure
    â”œâ”€â”€ Docker & Docker Compose
    â”œâ”€â”€ Health Monitoring
    â””â”€â”€ Logging & Metrics
```

## ğŸ“‹ Prerequisites

- Python 3.11+
- PostgreSQL 15+
- Redis 7+
- OpenAI API Key
- Pinecone API Key (for vector database)
- Docker & Docker Compose (for containerized deployment)

## ğŸ› ï¸ Installation

### Option 1: Docker Deployment (Recommended)

1. **Clone and navigate to the AI service directory:**
   ```bash
   cd ai-service
   ```

2. **Set up environment variables:**
   ```bash
   cp env.example .env
   # Edit .env and add your OpenAI API key
   ```

3. **Start the services:**
   ```bash
   # Start core services
   docker-compose up -d postgres redis ai_service pgadmin
   
   # Start with vector store (optional)
   docker-compose --profile vector-store up -d
   
   # Start with monitoring (optional)
   docker-compose --profile monitoring up -d
   ```

4. **Verify the installation:**
   ```bash
   curl http://localhost:8003/health
   ```

### Option 2: Local Development

1. **Create virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables:**
   ```bash
   cp env.example .env
   # Edit .env with your configuration
   ```

4. **Start PostgreSQL and Redis:**
   ```bash
   docker-compose up -d postgres redis
   ```

5. **Run the service:**
   ```bash
   python main.py
   ```

## ğŸ”§ Configuration

### Environment Variables

Key configuration options in `.env`:

```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Database
DATABASE_URL=postgresql://ai_user:ai_password@localhost:5434/ai_service_db

# AI Models
DEFAULT_EMBEDDING_MODEL=text-embedding-3-small
DEFAULT_CHAT_MODEL=gpt-4o-mini

# Service Configuration
AI_SERVICE_PORT=8003
LOG_LEVEL=INFO
```

### Database Setup

The service automatically creates all necessary tables on startup. For manual setup:

```bash
# Connect to PostgreSQL
psql -h localhost -p 5434 -U ai_user -d ai_service_db

# Or use pgAdmin at http://localhost:8083
# Email: admin@ai-service.com
# Password: admin123
```

## ğŸ“š API Documentation

### Base URL
```
http://localhost:8003
```

### Interactive Documentation
- **Swagger UI**: http://localhost:8003/docs
- **ReDoc**: http://localhost:8003/redoc

### Core Endpoints

#### 1. Job Match Analysis
```http
POST /api/ai/analyze-match
Content-Type: application/json

{
  "resume_text": "Your resume content...",
  "job_description": "Job description content...",
  "include_detailed_analysis": true,
  "use_enhanced_analysis": false
}
```

#### 2. Resume Optimization
```http
POST /api/ai/optimize-resume
Content-Type: application/json

{
  "resume_text": "Original resume content...",
  "job_description": "Target job description...",
  "optimization_level": "standard",
  "include_analysis": true
}
```

#### 3. Enhanced Optimization (3-Stage Pipeline)
```http
POST /api/ai/enhanced-optimization
Content-Type: application/json

{
  "resume_text": "Original resume content...",
  "job_description": "Target job description...",
  "user_context": {
    "industry": "technology",
    "experience_level": "senior"
  },
  "optimization_focus": ["leadership", "technical_skills"]
}
```

#### 4. Batch Job Analysis
```http
POST /api/ai/batch-analyze
Content-Type: application/json

{
  "resume_text": "Your resume content...",
  "job_descriptions": [
    "Job description 1...",
    "Job description 2...",
    "Job description 3..."
  ],
  "include_rankings": true,
  "parallel_processing": true
}
```

#### 5. Generate Embeddings
```http
POST /api/ai/embeddings
Content-Type: application/json

{
  "texts": [
    "Text to embed 1",
    "Text to embed 2"
  ],
  "model": "text-embedding-3-small",
  "store_embeddings": true
}
```

### Session Management

#### Get Session Status
```http
GET /api/ai/session/{session_id}/status
```

#### List User Sessions
```http
GET /api/ai/sessions?user_id={user_id}&page=1&page_size=20
```

### Health & Monitoring

#### Health Check
```http
GET /api/ai/health
```

### Cache Management

#### Get Cache Statistics
```http
GET /api/ai/cache/stats
```

#### Check Cache Health
```http
GET /api/ai/cache/health
```

#### Clear Cache by Type
```http
DELETE /api/ai/cache/clear/{prefix}
```

Available prefixes: `embedding`, `openai_response`, `job_match`, `optimization`, `session`, `health`

#### Clear All Cache
```http
DELETE /api/ai/cache/clear-all
```

#### Usage Metrics
```http
GET /api/ai/metrics
```

#### Available Models
```http
GET /api/ai/models
```

### Pinecone Vector Database Endpoints

#### Vector Similarity Search
```http
POST /api/pinecone/search
Content-Type: application/json

{
  "query_text": "Python developer with React experience",
  "index_type": "jobs",
  "top_k": 10,
  "min_similarity": 0.7,
  "filter_metadata": {
    "location": "San Francisco"
  }
}
```

#### Upsert Vector
```http
POST /api/pinecone/upsert
Content-Type: application/json

{
  "content_id": "resume_123",
  "content_text": "Senior Software Engineer with 5+ years...",
  "content_type": "resume",
  "user_id": "user_456",
  "metadata": {
    "experience_years": 5,
    "skills": ["Python", "React", "AWS"]
  }
}
```

#### Enhanced Job Matching
```http
POST /api/pinecone/match-resume-to-jobs
Content-Type: application/json

{
  "resume_text": "Senior Software Engineer with 5+ years...",
  "job_ids": ["job_1", "job_2"],
  "top_k": 20,
  "min_similarity": 0.7,
  "user_id": "user_123"
}
```

#### Index Statistics
```http
GET /api/pinecone/stats/{index_type}
```

#### Health Check
```http
GET /api/pinecone/health
```

## ğŸ—„ï¸ Pinecone Vector Database Integration

### Overview

The AI Service includes comprehensive Pinecone vector database integration for high-performance semantic search and job matching:

#### Key Features
- **Vector Similarity Search**: Sub-millisecond semantic search using OpenAI embeddings
- **Job-Resume Matching**: Intelligent matching combining vector similarity and semantic analysis
- **Batch Operations**: Efficient processing of large datasets
- **Metadata Filtering**: Advanced filtering and querying capabilities
- **Real-time Indexing**: Automatic index creation and management
- **Health Monitoring**: Comprehensive health checks and statistics

#### Performance Benefits
- **95%+ faster search** compared to traditional keyword matching
- **Scalable to millions** of resumes and job descriptions
- **Real-time updates** with automatic index synchronization
- **High availability** with Pinecone's managed infrastructure

### Setup

1. **Get Pinecone API Key**: Sign up at [pinecone.io](https://pinecone.io)
2. **Configure Environment**: Add to your `.env` file:
   ```bash
   PINECONE_API_KEY=your_pinecone_api_key_here
   PINECONE_ENVIRONMENT=us-west1-gcp
   ```
3. **Automatic Index Creation**: The service creates required indexes on startup

### Usage Examples

```python
import asyncio
from app.services.pinecone_service import pinecone_service

async def example():
    # Add resume to vector database
    await pinecone_service.upsert_resume(
        resume_id="resume_123",
        resume_text="Senior Software Engineer with 5+ years...",
        user_id="user_456"
    )
    
    # Search for similar jobs
    results = await pinecone_service.search_similar_jobs(
        query_text="Python developer with React experience",
        top_k=10
    )
    
    # Match resume to jobs
    matches = await pinecone_service.match_resume_to_jobs(
        resume_text="Senior Software Engineer with 5+ years...",
        top_k=20,
        min_similarity=0.7
    )

# Run example
asyncio.run(example())
```

For detailed documentation, see [PINECONE_INTEGRATION.md](PINECONE_INTEGRATION.md).

## âš¡ Performance & Caching

### Redis Caching Benefits

The AI service implements comprehensive Redis caching for optimal performance:

#### Performance Improvements
- **Embedding Generation**: 95%+ faster for cached text
- **OpenAI Responses**: 95%+ faster for similar prompts
- **Job Match Analysis**: 95%+ faster for repeated analysis
- **Resume Optimization**: 90%+ faster for similar optimizations

#### Cost Reduction
- **API Call Reduction**: 75% fewer OpenAI API calls for typical usage
- **Embedding Savings**: 80-90% reduction in embedding generation costs
- **Response Time**: Near-instantaneous results for cached data

#### Cache Configuration
- **Embeddings**: 7-day TTL (expensive to generate, rarely change)
- **OpenAI Responses**: 6-hour TTL (may vary slightly but expensive)
- **Job Match Results**: 1-hour TTL (user-specific)
- **Optimization Results**: 1-hour TTL (user-specific)

#### Cache Management
```bash
# Check cache statistics
curl http://localhost:8003/api/ai/cache/stats

# Monitor cache health
curl http://localhost:8003/api/ai/cache/health

# Clear specific cache types
curl -X DELETE http://localhost:8003/api/ai/cache/clear/embedding
curl -X DELETE http://localhost:8003/api/ai/cache/clear/openai_response

# Clear all cache
curl -X DELETE http://localhost:8003/api/ai/cache/clear-all
```

## ğŸ’¡ Usage Examples

### Python Client Example

```python
import requests
import json

# Base URL
BASE_URL = "http://localhost:8003"

# Job match analysis
def analyze_job_match(resume_text, job_description):
    response = requests.post(
        f"{BASE_URL}/api/ai/analyze-match",
        json={
            "resume_text": resume_text,
            "job_description": job_description,
            "include_detailed_analysis": True
        }
    )
    return response.json()

# Resume optimization
def optimize_resume(resume_text, job_description):
    response = requests.post(
        f"{BASE_URL}/api/ai/optimize-resume",
        json={
            "resume_text": resume_text,
            "job_description": job_description,
            "optimization_level": "enhanced"
        }
    )
    return response.json()

# Enhanced optimization
def enhanced_optimization(resume_text, job_description):
    response = requests.post(
        f"{BASE_URL}/api/ai/enhanced-optimization",
        json={
            "resume_text": resume_text,
            "job_description": job_description,
            "user_context": {"industry": "technology"}
        }
    )
    return response.json()

# Example usage
resume = "Experienced software engineer with 5+ years..."
job_desc = "Senior Software Engineer position..."

# Analyze match
match_result = analyze_job_match(resume, job_desc)
print(f"Match Score: {match_result['match_scores']['overall']}")

# Optimize resume
optimized = optimize_resume(resume, job_desc)
print(f"Optimized Resume: {optimized['optimized_resume']}")
```

### cURL Examples

```bash
# Health check
curl http://localhost:8003/health

# Job match analysis
curl -X POST http://localhost:8003/api/ai/analyze-match \
  -H "Content-Type: application/json" \
  -d '{
    "resume_text": "Experienced software engineer...",
    "job_description": "Senior Software Engineer position..."
  }'

# Resume optimization
curl -X POST http://localhost:8003/api/ai/optimize-resume \
  -H "Content-Type: application/json" \
  -d '{
    "resume_text": "Original resume content...",
    "job_description": "Target job description..."
  }'
```

## ğŸ” Database Schema

### Core Tables

- **ai_processing_sessions**: Track processing sessions
- **embeddings**: Store vector embeddings
- **resume_optimizations**: Store optimization results
- **job_match_analyses**: Store job matching results
- **vector_indices**: Vector search indices
- **ai_usage_metrics**: Usage analytics
- **ai_service_config**: Service configuration

### Sample Queries

```sql
-- Get recent optimizations
SELECT * FROM resume_optimizations 
ORDER BY created_at DESC 
LIMIT 10;

-- Get high-confidence results
SELECT * FROM resume_optimizations 
WHERE confidence_score >= 80.0;

-- Get session statistics
SELECT 
    operation_type,
    COUNT(*) as total_sessions,
    AVG(processing_time_ms) as avg_processing_time
FROM ai_processing_sessions 
GROUP BY operation_type;
```

## ğŸ§ª Testing

### Run Tests
```bash
# Install test dependencies
pip install pytest pytest-asyncio pytest-cov

# Run tests
pytest tests/ -v --cov=app

# Run with coverage report
pytest tests/ --cov=app --cov-report=html
```

### Test API Endpoints
```bash
# Test health endpoint
curl http://localhost:8003/health

# Test job matching
curl -X POST http://localhost:8003/api/ai/analyze-match \
  -H "Content-Type: application/json" \
  -d '{
    "resume_text": "Software engineer with Python experience",
    "job_description": "Python developer position"
  }'
```

## ğŸ“Š Monitoring

### Health Checks
- **Service Health**: `GET /health`
- **AI Service Health**: `GET /api/ai/health`
- **Database Health**: Automatic checks in health endpoints

### Metrics
- **Usage Metrics**: `GET /api/ai/metrics`
- **Database Stats**: Automatic collection
- **Performance Metrics**: Processing times, success rates

### Logs
```bash
# View service logs
docker-compose logs ai_service

# View database logs
docker-compose logs postgres

# Follow logs in real-time
docker-compose logs -f ai_service
```

## ğŸ”§ Development

### Project Structure
```
ai-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ai_models.py      # Database models
â”‚   â”‚   â””â”€â”€ schemas.py        # Pydantic schemas
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ ai_routes.py      # API routes
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai_service.py     # Core AI service
â”‚   â”‚   â””â”€â”€ enhanced_optimization.py  # 3-stage pipeline
â”‚   â”œâ”€â”€ utils/                # Utility functions
â”‚   â””â”€â”€ validators/           # Input validation
â”œâ”€â”€ tests/                    # Test files
â”œâ”€â”€ logs/                     # Application logs
â”œâ”€â”€ data/                     # Data storage
â”œâ”€â”€ main.py                   # FastAPI application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ docker-compose.yml       # Service orchestration
â””â”€â”€ README.md               # This file
```

### Adding New Features

1. **Add Database Models** (`app/models/ai_models.py`)
2. **Create Pydantic Schemas** (`app/models/schemas.py`)
3. **Implement Service Logic** (`app/services/`)
4. **Add API Routes** (`app/routers/ai_routes.py`)
5. **Write Tests** (`tests/`)
6. **Update Documentation**

### Code Style
```bash
# Install development tools
pip install black flake8

# Format code
black app/ tests/

# Lint code
flake8 app/ tests/
```

## ğŸš€ Deployment

### Production Deployment

1. **Environment Setup:**
   ```bash
   # Production environment variables
   ENVIRONMENT=production
   DEBUG=false
   LOG_LEVEL=WARNING
   ```

2. **Database Migration:**
   ```bash
   # The service auto-creates tables, but you can use Alembic for migrations
   alembic upgrade head
   ```

3. **Docker Production Build:**
   ```bash
   docker-compose -f docker-compose.prod.yml up -d
   ```

### Scaling

- **Horizontal Scaling**: Run multiple AI service instances behind a load balancer
- **Database Scaling**: Use PostgreSQL read replicas for read-heavy workloads
- **Caching**: Redis for session storage and result caching
- **Vector Store**: ChromaDB or FAISS for large-scale embedding storage

## ğŸ¤ Integration

### With Other Services

- **User Service** (Port 8001): User authentication and management
- **Resume Service** (Port 8002): Resume file processing and storage
- **Main Backend** (Port 8000): Orchestration and business logic

### API Integration Example

```python
# Integration with resume service
def process_resume_with_ai(resume_id, job_description):
    # Get resume from resume service
    resume_response = requests.get(f"http://localhost:8002/api/resumes/{resume_id}")
    resume_text = resume_response.json()["content"]
    
    # Process with AI service
    ai_response = requests.post(
        "http://localhost:8003/api/ai/enhanced-optimization",
        json={
            "resume_text": resume_text,
            "job_description": job_description
        }
    )
    
    # Store optimized resume back to resume service
    optimized_resume = ai_response.json()["optimized_resume"]
    requests.put(
        f"http://localhost:8002/api/resumes/{resume_id}/optimized",
        json={"optimized_content": optimized_resume}
    )
    
    return ai_response.json()
```

## ğŸ› Troubleshooting

### Common Issues

1. **OpenAI API Key Missing**
   ```bash
   # Set your OpenAI API key
   export OPENAI_API_KEY="your-api-key-here"
   ```

2. **Database Connection Issues**
   ```bash
   # Check database status
   docker-compose ps postgres
   
   # Check database logs
   docker-compose logs postgres
   ```

3. **Service Not Starting**
   ```bash
   # Check service logs
   docker-compose logs ai_service
   
   # Check health endpoint
   curl http://localhost:8003/health
   ```

4. **Memory Issues**
   ```bash
   # Increase Docker memory limit
   # In Docker Desktop: Settings > Resources > Memory
   ```

### Debug Mode

```bash
# Enable debug mode
export DEBUG=true
export LOG_LEVEL=DEBUG

# Restart service
docker-compose restart ai_service
```

## ğŸ“ˆ Performance

### Optimization Tips

1. **Batch Processing**: Use batch analysis for multiple jobs
2. **Caching**: Enable Redis caching for repeated requests
3. **Connection Pooling**: Configure database connection pools
4. **Async Processing**: Use async endpoints for long-running operations

### Benchmarks

- **Job Match Analysis**: ~2-3 seconds
- **Resume Optimization**: ~5-8 seconds
- **Enhanced Optimization**: ~10-15 seconds
- **Batch Analysis (10 jobs)**: ~15-25 seconds

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ Support

- **Documentation**: Check the `/docs` endpoint for interactive API docs
- **Issues**: Report bugs and feature requests via GitHub issues
- **Discussions**: Use GitHub discussions for questions and ideas

## ğŸ”„ Changelog

### Version 1.0.0
- Initial release
- Core AI functionality
- Enhanced optimization pipeline
- Vector embeddings support
- Session management
- Health monitoring
- Docker deployment
